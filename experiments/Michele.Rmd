---
title: "Linear Models"
author: "Elena Rivaroli"
date: "2023-01-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(plotly)
library(gridExtra)
```

# Import the dataset:

```{r cars}
car <- read.csv("./car.csv")
# Remove observation 1080
car <- car[car$Kilometer != 2000.000,]
```

```{r}
library(skimr)
skim(car)
```

# Correlation plot
```{r}
cor_mat <- cor(car[,unlist(lapply(car, is.numeric), use.names = FALSE)],use = "complete.obs")
data<-expand.grid(X=unlist(dimnames(cor_mat)[2]),Y=unlist(dimnames(cor_mat)[2]))
data$Z<- c(cor_mat)

#ggplot(data,aes(X,Y,fill=Z))+geom_tile()+theme(axis.text.x=element_text(angle = -90, hjust = 0))+coord_equal()+scale_y_discrete(limits=rev)+geom_text(aes(label=prettyNum(Z,digits=2,format="f")), color="white", size=4)

library(corrplot)
corrplot(cor_mat, type = "upper")
```

# Log Price vs Make
```{r}
car %>%
  ggplot( aes(x=Make, y=log(Price), fill=Make)) +
    geom_boxplot() +
    geom_jitter(color="black", size=0.2, alpha=0.5) +
    theme(axis.text.x=element_text(angle = -90, hjust = 0)) +
    ggtitle("Log Price vs Make") + 
    xlab("") + ylab("Log Price")
```

# Log Price vs Owner
```{r}
library(dplyr)
car %>%
  ggplot( aes(x=Owner, y=log(Price), fill=Owner)) +
    geom_boxplot() +
    
    geom_jitter(color="black", size=0.2, alpha=0.5) +
  
    ggtitle("Log Price vs Owner") +
    xlab("")
```



# Distribution of categorical variable
```{r} 
p12 <- ggplot(car, aes(x=reorder(Make, Make, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Make')+
  ggtitle("Distribution of Make")+theme_minimal()+theme(axis.text.x = element_text(angle = 90))

p13 <- ggplot(car, aes(x=reorder(Fuel.Type, Fuel.Type, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Fuel Type')+
  ggtitle("Distribution of Fuel Type")+theme_minimal()

p14 <- ggplot(car, aes(x=reorder(Transmission,Transmission, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Transmission')+
  ggtitle("Distribution of Transmission")+theme_minimal()

p15 <- ggplot(car, aes(x=reorder(Owner,Owner, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Owner')+
  ggtitle("Distribution of Owner")+theme_minimal()

p16 <- ggplot(car, aes(x=reorder(Seller.Type, Seller.Type, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Seller Type')+
  ggtitle("Distribution of Seller Type")+theme_minimal()

p17 <- ggplot(car, aes(x=reorder(Drivetrain, Drivetrain, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Drivetrain')+
  ggtitle("Distribution of Drivetrain")+theme_minimal()

p18 <- ggplot(car, aes(x=reorder(SeatingCapacity, SeatingCapacity, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Seating Capacity')+
  ggtitle("Distribution of Seating Capacity")+theme_minimal()

grid.arrange(p13, p14,p15, p16, p17, p18, ncol=3)
```

We start to study the relationship of the dependent variable with the other numerical ones

# Price vs numerical variables
```{r}
car1 <- car[car$Kilometer !=925.000, ]
p12 <- ggplot(car, aes(x = Year, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) over the years")+ theme_classic()

p13 <-ggplot(car1, aes(x = Kilometer, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs travelled Kilometer")+ theme_classic() 

p14 <- ggplot(car, aes(x = Engine, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs Engine")+ theme_classic()

p15 <- ggplot(car, aes(x = Power, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs Max Power")+ theme_classic()

p16 <- ggplot(car, aes(x = Torque, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs Max Torque")+ theme_classic()

p17 <- ggplot(car, aes(x = Length, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs Length")+ theme_classic()

p18 <- ggplot(car, aes(x = Width, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs Width")+ theme_classic()

p19 <- ggplot(car, aes(x = Height, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs Height")+ theme_classic()

p20 <- ggplot(car, aes(x = TankCapacity, y =log(Price))) +
    geom_point(size=0.7)+
  ggtitle("Distribution of log(Price) vs Fuel Tank Capacity")+ theme_classic()

grid.arrange(p12, p13, p14,p15, p16, p17, p18, p19, p20, ncol=3)
```

We can observe an increasing trend in price for increasing value of other variables, except for the number of kilometer for whixh a very slow decreasing trend can be spotted. Instead for what concernes dimensions of the car, lenght and height seems to have a posite relationship with car price, but no significant pattern can be spotted in the Price vs Height plot (we can exclude the variable from the model)


# Kilometer and logPrice analysis
From the following scatterplot we can see a very small decreasing trend in the log of Prices as the number of kilometers increases, but most important we can see that, in general, for a similar number of km, older cars (darker points) have a lower price than newer ones (light points) (as expected)
```{r}
car1 <- car[car$Kilometer !=925.000, ]
pl1 <- ggplot(car1, aes(x=Kilometer, y=log(Price)))+
  geom_point(aes(col=Year))+theme_classic()
ggplotly(pl1)
```

same thing can be said for what refers to brand. For two cars having the same number of km, the brand makes a great difference in the log of price (Tata with 60 000km costs 10.79 (da trasformare), while a Rolls-Royce with same number of kilometer costs 16.70)

```{r}
pl2 <- ggplot(car1, aes(x=Kilometer, y=log(Price)))+
  geom_point(aes(col=Make))+theme_classic()
ggplotly(pl2)
```


```{r}
pl3 <- ggplot(car1, aes(x=Year, y=log(Price)))+
  geom_point(aes(col=Make))+theme_classic()
ggplotly(pl3)
```

We study also correlation of the numerical variables with Price and among themselves

```{r}
library(corrplot)
M <- round(cor(car[, c(2, 3, 4, 9:11, 13:15, 17)]),2)
corrplot(M, method="number", number.cex = 0.7, tl.col = "black")

```

According to the corrplot, I'd delete the Height variable since it'is not correlated with Price and add not a real pattern with log(Price), but I'd lascerei year and km, since they showed a clear trend in the scatter plots. For what concernes correlated features, I'd deleet torque and Engine, since they are highly correlated with Power.

# Esperimento di data visualization
```{r}
# capisco dove ci sono piu dati
p<-ggplot(car, aes(x=Kilometer, y=log(Price) )) +
  geom_bin2d(bins = 30) +
  scale_fill_continuous(type = "viridis") +
  theme_bw()

ggplotly(p)
```


```{r}
# analizzo quella fetta di dati
p<-ggplot(car[ car$Kilometer < 100 & car$Kilometer >50 & car$Price>exp(13) & car$Price< exp(15),], aes(x=Kilometer, y=log(Price), size = Engine, color = Fuel.Type)) +
    geom_point(alpha=0.5) +
  scale_size(range = c(.2, 7), name="Engine")
ggplotly(p)
```


```
# Charge libraries:
library(ggplot2)
library(gganimate)
library(gifski)
 
# Make a ggplot, but add frame=year: one image per year
myPlot <- ggplot(car, aes(Kilometer, log(Price))) +
  geom_point(alpha = 0.5, show.legend = FALSE) +
  
  scale_x_log10() +
  facet_wrap(~Fuel.Type) +
  # Here comes the gganimate specific bits
  labs(title = 'Year: {frame_time}', x = 'Kilometer', y = 'Price') +
  transition_time(Year) +
  ease_aes('linear')

# Save at gif:


animate(myPlot, duration = 16, fps = 30, width = 200, height = 200, renderer = gifski_renderer())
anim_save("output.gif")

library(ggplot2)
library(gganimate)
library(dplyr)


# Keep only 3 names
don <- car %>% 
  filter(Fuel.Type %in% c("Diesel", "Petrol", "Other"))
  
# Plot
myPlot <- don %>%
  ggplot( aes(x=Year, y=log(Price), group=Fuel.Type, color=Fuel.Type)) +
    geom_line() +
    geom_point() +
    ggtitle("Selling Price wrt Fuel Type") +
    ylab("Log Price") +
    transition_reveal(Year)

animate(myPlot, duration = 10, fps = 40, width = 200, height = 200, renderer = gifski_renderer())
anim_save("output1.gif")
```


# Linear Model
```{r cars}
set.seed(1)
index <- sample(nrow(car), nrow(car)*0.8, replace=F)
train <- car[index,]
test <- car[-index,]

```

Fit a simple linear model with all the covariates:
```{r}
lm <- lm(Price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+as.factor(Seller.Type)+Engine+Power+Torque+
         as.factor(Drivetrain)+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, data=train)
summary(lm)
```

Let's see the R^2:

```{r}
# R-squared
summary(lm)$adj.r.squared
```
The adjusted $R^2$ is 0.7904685
Looking at the residuals plot vs the continuous explanatory variable is a good check to highlight (if there are) structures/patterns in the residuals:

```{r}
ggplot(data=train,aes(x = Price, y =  lm$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  geom_smooth(color = '#56B4E9', se = F) +
  labs(x = 'Price', y = 'Residuals', title = 'Fitted vs. Observed') +
  #hrbrthemes::theme_ipsum_rc() +
  theme(title = element_text(colour = 'black'))
```
We can see that the mean is almost 0 but there is a linear relationship with the price.
Let's see the residuals plot:

```{r}
# Plot the residuals and the fitted values.
plot(
  lm,
  which = 1,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 2,
  cex = .95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')
```
```{r}
plot(
  lm,
  which = 2,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')
```


```{r}

# si può togliere
par(mfrow=c(2,2))

plot(
  lm,
  which = 1,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')

plot(
  lm,
  which = 2,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')

plot(
  lm,
  which = 3,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')

plot(
  lm,
  which = 5,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')
```



```{r}
# Look the histograms of price and km
par(mfrow=c(1,2))
hist(train$Price, probability=TRUE, breaks=15)
hist(train$Kilometer, probability=TRUE, breaks=15)
```

The two distributions have a long right tail, and lot of values are shrunk towards zero. Furthermore, the extreme points on the right tails influence a lot the estimation of the equation line, they have large leverage. Maybe, we need a more symmetric distribution, such as the logarithm.

```{r}
# Transform the variable in the log-scale 
#(add the variables to the car data.frame)
train$log_price <- log(train$Price)
#train$log_km <- log(train$km)
```


```{r}
# Look the histograms of log_price and log_k
```

# Look the histograms of log_price and log_km
```{r}
par(mfrow=c(1,2))
ggplot(train, aes(x = Price)) + 
  geom_histogram(aes(y = ..density..),
                 colour = 1, fill = "white") +
  geom_density(lwd = 1, colour = 4,
               fill = 4, alpha = 0.25) +
  xlab("Price") + ylab("Density")

ggplot(train, aes(x = log_price)) + 
  geom_histogram(aes(y = ..density..),
                 colour = 1, fill = "white") +
  geom_density(lwd = 1, colour = 4,
               fill = 4, alpha = 0.25) +
  xlab("Log Price") + ylab("Density")
```

Fit the model with logarithm of price:

```{r}
lm1 <- lm(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+as.factor(Seller.Type)+Engine+Power+Torque+
         as.factor(Drivetrain)+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, data=train)

summary(lm1)
```

Now, more covariates are significant and also the results are better:
  - Residual standard error is 0.2068
  - adjusted $R^$ is 0.9554 (previously it was 0.7905)
  - F-statistics is 644.5 (previously 142.8)

```{r}
AIC(lm1)
```

```{r}
library(MASS) # for hist.scott
par(mfrow=c(1,1))

ggplot(data=train,aes(x = log_price, y = lm1$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Log Price', y = "Residuals", title = 'Fitted vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

ggplot(data=train,aes(x = c(1:nrow(train)), y = lm1$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = "",y = 'Residuals', title = 'Residuals') +
  theme(title = element_text(colour = '#585858'))

ggplot(train, aes(x = lm1$residuals)) + 
  geom_histogram(aes(y = ..density..),
                 colour = 1, fill = "white") +
  geom_density(lwd = 1, colour = 4,
                 alpha = 0.2) +
  xlab("Log Price") + ylab("Density")  +
stat_function(fun = dnorm, 
              args = list(mean = 0, sd = sd(lm1$residual)),
              colour =2,
              lwd = 1,
              alpha = 1)


```

Also the residual plot is better.

```{r}
par(mfrow=c(2,2))

plot(
  lm1,
  which = 1,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')

plot(
  lm1,
  which = 2,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')

plot(
  lm1,
  which = 3,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')

plot(
  lm1,
  which = 4,
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')
```

```{r}
par(mfrow=c(2,2))
plot(lm1,
     bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n')
```



In the normal Q-Q plot we can see that there are heavy tails.

We can try to remove engine and torque, which are very positive correlated with power.
Power is the only significant variable among these three.


```{r}
lm_pwr <- lm(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+as.factor(Seller.Type)+Power+
         as.factor(Drivetrain)+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, data=train)

summary(lm_pwr)
```
  - Residual standard error: 0.2068
  - Adjusted R-squared:  0.9554
  - F-statistic: 670.1

```{r}
AIC(lm_pwr)
```
```{r}
anova(lm_pwr,lm1)
```


```{r}
ggplot(data=train,aes(x = log_price, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Log Price', y = "Residuals", title = 'Log Price vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

ggplot(data=train,aes(x = c(1:nrow(train)), y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = "",y = 'Residuals', title = 'Residuals') +
  theme(title = element_text(colour = '#585858'))

ggplot(train, aes(x = lm_pwr$residuals)) + 
  geom_histogram(aes(y = ..density..),
                 colour = 1, fill = "white") +
  geom_density(lwd = 1, colour = 4,
                 alpha = 0.2) +
  xlab("Log Price") + ylab("Density")  +
stat_function(fun = dnorm, 
              args = list(mean = 0, sd = sd(lm1$residual)),
              colour =2,
              lwd = 1,
              alpha = 1)
```

```{r}
d<-density(lm_pwr[['residuals']])
plot(d,main='Residuals Plot',xlab='Residual value')
```
The tails are not perfect but not so bad.

Let's plot "fitted vs true value"
```{r}
ggplot(data=train,aes(x = predict(lm_pwr), y = log_price)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Log Price', y = "Residuals", title = 'Log Price vs. Residuals') +
  theme(title = element_text(colour = '#585858'))
```
```{r}
ggplot(data=train,aes(x = seq(nrow(train)), y = log_price)) +
  geom_point(alpha = .5, color = '#D55E00') +
  geom_point(aes(x = seq(nrow(train)), y = predict(lm_pwr) ), color = '#53004B', alpha=.5) +
  xlab("") + ylab("") +
  theme(title = element_text(colour = '#585858'))
```


```{r}
par(mfrow=c(2,2))
plot(lm_pwr,
      bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n')
```


From the first plot (residuals vs fitted), we can observe that for lower values of the price, the mean of the residuals is a little bit higher than 0. But it's ok!!
In the normal Q-Q plot we can see that there are heavy tails (this is to investigate a little bit).

Let's try to plot residuals vs all the covariates.
```{r}
par(mfrow=c(1,1))
plot(as.factor(train$Make), lm_pwr$residuals)

# Year vs Res

ggplot(data=train,aes(x = Year, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Year', y = "Residuals", title = 'Year vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# Km vs Res

ggplot(data=train,aes(x = Kilometer, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Kilometer', y = "Residuals", title = 'Kilometer vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# Engine vs Res

ggplot(data=train,aes(x = Engine, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Engine', y = "Residuals", title = 'Engine vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# Power vs Res

ggplot(data=train,aes(x = Power, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Power', y = "Residuals", title = 'Power vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# Torque vs Res

ggplot(data=train,aes(x = Torque, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Torque', y = "Residuals", title = 'Torque vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# Length vs Res

mygroup <- c(0,4200, max(train$Length))
ggplot(data=train,aes(x = Length, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, aes(colour =  cut(Length,breaks = mygroup) ), , show.legend=F) +
  labs(x = 'Length', y = "Residuals", title = 'Length vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# Width vs Res

ggplot(data=train,aes(x = Width, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Width', y = "Residuals", title = 'Width vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# Height vs Res

ggplot(data=train,aes(x = Height, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'Height', y = "Residuals", title = 'Height vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

# TankCapacity vs Res

ggplot(data=train,aes(x = TankCapacity, y = lm_pwr$residuals)) +
  geom_point(alpha = .25, color = '#D55E00') +
  labs(x = 'TankCapacity', y = "Residuals", title = 'TankCapacity vs. Residuals') +
  theme(title = element_text(colour = '#585858'))

```

Pattern in length???
Except in kilometer, where we can see a point with a very high leverage (outlier?).

CHECKPOINT


Let's try with lasso regression:

```{r}
library(glmnet)
xx <- model.matrix(log_price~.-log_price-Price-Engine-Torque, data=train)
fit.lasso <- glmnet(xx,train$log_price, alpha=1) # alpha=1 selects lasso
plot(fit.lasso,"lambda")
```


```{r}
set.seed(1)
cv_model <- cv.glmnet(xx, train$log_price, alpha = 1)
plot(cv_model)

best_lambda <- cv_model$lambda.1se

best_model <- glmnet(xx, train$log_price, alpha = 1, lambda = best_lambda)
summary(best_model)
coef(best_model)
best_model
```

Let's try with ridge regression:

```{r}
library(MASS)
# Tuning parameter = 0 implies least square estimates

lm_ridge<- lm.ridge(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+as.factor(Seller.Type)+Power+Engine+Torque+
         as.factor(Drivetrain)+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, lambda=0, data=train)
coef(lm_ridge)
# select lambda by GCV in the model with logLDC
grid.ridge<-lm.ridge(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+as.factor(Seller.Type)+Power+Engine+Torque+
         as.factor(Drivetrain)+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity,lambda=seq(0.1,10,0.001), data=train)

lambda_selected<-grid.ridge$lambda[which(grid.ridge$GCV==min(grid.ridge$GCV))]

lm_ridge_GCV <- lm.ridge(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+as.factor(Seller.Type)+Power+Engine+Torque+
         as.factor(Drivetrain)+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, lambda=lambda_selected, data=train)
coef(lm_ridge_GCV)
summary(lm_ridge)
```


Linear model without make:

```{r}
lm_noMake <- lm(log_price ~ Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+as.factor(Seller.Type)+Power+as.factor(Drivetrain)+
           Length+Width+Height+as.factor(SeatingCapacity)+TankCapacity, data=train)
summary(lm_noMake)
```
```{r}
AIC(lm_noMake)
```

```{r}
anova(lm_noMake,lm_pwr)
```


```{r}
par(mfrow=c(1,1))
plot(train$log_price, lm_noMake$residuals)
```

```{r}
par(mfrow=c(2,2))
plot(lm_noMake,
      bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n')
```


Let's try to remove by hand not significant variables from the first model (with logarithm):

```{r}
summary(lm_pwr)
```

```{r}
library(car)
vif(lm1)
vif(lm_pwr)
```

We can see that both levels of "seller" are not significant, let's try to remove it

```{r}
lm3 <- lm(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+Power+as.factor(Drivetrain)+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, data=train)
summary(lm3)

AIC(lm3)
anova(lm3, lm_pwr)
```
Ci piace di più quello ridotto.

```{r}
par(mfrow=c(1,1))
plot(lm3,
      bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n')
```


Still both levels of "drivetrain" are not significant:

```{r}
lm4 <- lm(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+Power+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, data=train)
summary(lm4)
AIC(lm4)
anova(lm4,lm3)
vif(lm4)
```

```{r}
par(mfrow=c(1,1))
plot(lm4,
      bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 1,
  cex =.95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n')
```
Try to remove another explanatory variable, for example "Seating Capacity"

```{r}
lm6 <- lm(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+Power+Length+Width+Height+TankCapacity, data=train)
summary(lm6)
AIC(lm4)
AIC(lm6)
anova(lm6,lm4)
```
We can see that the more complex model is better!!

Just to try, I remove a significant covariate, for example "Length" from lm3:

```{r}
lm5 <- lm(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+Power+as.factor(Drivetrain)+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity, data=train)
summary(lm5)
anova(lm5,lm3)

# We can see that the p-value is low so lm3 is better than lm5, this means that Length is significant
```


Just do the test on the test set with lm4 (without Engine, Torque, Seller Type and Drive Train):

```{r}
summary(lm4)
test$log_price <- log(test$Price)
predicted_test <- predict(lm4, newdata = test)

ggplot(data=test,aes(x = predicted_test, y = test$log_price)) +
  geom_abline(intercept = 0, 
              slope = 1, 
              color="black", 
              size=.5,
              alpha = .5) +
  geom_point(alpha = .75, 
             color = '#D55E00') +
  labs(x = 'Predicted', 
       y = "Actual", 
       title = '') +
  theme(title = element_text(colour = '#585858')) 

#plot(exp(predicted_test), exp(test$log_price))

sum(abs(exp(predicted_test)-exp(test$log_price))/exp(test$log_price))/length(test$log_price)
```

```{r}
larger.res <- which(abs(lm4$residuals)>0.5)
df_res <- train[larger.res,]
```


```{r}
#print(sort(predicted_test-test$log_price))
```

Model 4 with interactions:
```{r}
lm4_interactions <- lm(log_price ~ as.factor(Make)+Year+Kilometer+as.factor(Fuel.Type)+as.factor(Transmission)+
         as.factor(Owner)+Power+Length+Width+Height+as.factor(SeatingCapacity)
         +TankCapacity+Year:Kilometer, data=train)
summary(lm4_interactions)
```



```{r}
plot(
  lm4,
  which = c(1,2,4,5),
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  #col.smooth = '#56B4E9',
  lwd = 2,
  cex = .95,
  col.lab = 'gray25',
  col.axis = 'gray50',
  col.sub = 'gray50',
  cex.caption = 1,
  cex.oma.main = 1.25,
  yaxt = 'n'
)
axis(2, col = 'gray75', col.axis = 'gray33')
axis(1, col = 'gray75', col.axis = 'gray33')


```

```{r}
lm_step <- step(lm1, scale = 0,
     direction = c("both", "backward", "forward"),steps = 1000, k = 2)
summary(lm_step)
length(coefficients(lm_step))
length(coefficients(lm1))
```

With respect to the first model (lm1), the step function removes:
  - Seller type
  - Engine
  - Torque
  - Drive train
Like I did in the final lm4.

```{r}
final4_vs_step <- c(AIC(lm4), AIC(lm_step))
final4_vs_step
```

# RF

```{r}
## important: reload data

car<-read.csv("car.csv")

# Transform categorical variables into factors
car$Make <- as.factor(car$Make)
car$Fuel.Type <- as.factor(car$Fuel.Type)
car$Transmission <- as.factor(car$Transmission)
car$Owner <- as.factor(car$Owner)
car$Seller.Type<- as.factor(car$Seller.Type)
car$Drivetrain <- as.factor(car$Drivetrain)
car$SeatingCapacity <- as.factor(car$SeatingCapacity)

car <- car[car$Kilometer != 2000,]

set.seed(1)
index <- sample(nrow(car),nrow(car)*0.8,replace = F)
train <- car[index,]
test <- car[-index,]
```

```{r}
library(randomForest)
m1 <- randomForest(
  formula = Price ~ .,
  data = car
)
```

```{r}
(m1)
```

```{r}
plot(m1)
```

```{r}
# number of trees with lowest MSE
which.min(m1$mse)


# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])

```


library(rsample)
library(dplyr)
# create training and validation data
set.seed(123)
valid_split <- initial_split(car, .8)

# training data
train <- analysis(valid_split)

# test data
test <- assessment(valid_split)
x_test <- test[setdiff(names(test), "Price")]
y_test <- test$Price

rf_oob_comp <- randomForest(
  formula = Price ~ .,
  data    = train,
  xtest   = x_test,
  ytest   = y_test
)

# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")


```{r}
library(ggplot2)
p<-predict(m1,test)
plot(
  log(predict(m1,test)), log(test$Price),
  bty = 'n',
  pch = 19,
  col = scales::alpha('#D55E00', .1),
  xlab="Predicted",
  ylab="True"
)
abline(0,1, col=2)



err<-sum(abs(predict(m1,test)-test$Price)/(test$Price))/length(test$Price)
print(paste("Mean absolute relative error:", err))
```


```{r}
# tune the number of variable to use for each tree
features <- setdiff(names(car), "Price")

set.seed(123)

m2 <- tuneRF(
  x          = car[features],
  y          = car$Price,
  ntreeTry   = 500,
  mtryStart  = 2,
  stepFactor = 1.5,
  improve    = 0.001,
  trace      = T      # to show real-time progress 
)
```

```{r}
varImpPlot(m1)
```
