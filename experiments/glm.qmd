---
title: "glms"
format: revealjs
editor: visual
---

# GLM

## Motivation

Scatterplots for price vs. other variables show an increase in variance for higher prices.

```{r,echo=F}
car<-read.csv("car.csv")

# Transform categorical variables into factors
car$Make <- as.factor(car$Make)
car$Fuel.Type <- as.factor(car$Fuel.Type)
car$Transmission <- as.factor(car$Transmission)
car$Owner <- as.factor(car$Owner)
car$Seller.Type<- as.factor(car$Seller.Type)
car$Drivetrain <- as.factor(car$Drivetrain)
car$SeatingCapacity <- as.factor(car$SeatingCapacity)

car <- car[car$Kilometer != 2000,]

set.seed(1)
index <- sample(nrow(car),nrow(car)*0.8,replace = F)
train <- car[index,]
test <- car[-index,]
```

```{r}
par(mfrow=c(2,2))
means <- tapply(train$Price,train$Year,mean)
with(train,boxplot(Price~Year))
points(means,col=2,pch=20)
with(train,plot(Kilometer,Price))
with(train,plot(Power,Price))
with(train,plot(Length,Price))
```

## First model

-   The increase in variance suggests that the price follows a gamma distribution
-   Since the price is always going to be positive, we can fulfil the requirement that the mean be strictly positive by choosing a log link

We removed Torque and Engine, which are strongly correlated, and we fitted a model.

```{r}
glm_full <- glm(Price~.-Engine-Torque,data=train,family=Gamma(link="log"))
```

## First model

```{r}
summary(glm_full)
```

We can see that Drivetrain and Seller Type are not significant

## Assessment of assumptions

We can check that our assumptions hold by looking at the residuals

```{r}
library(ggplot2)
par(mfrow=c(2,2))
plot(glm_full,pch=20,col=alpha('orange',0.25),panel=function(x, y, col = par("col"), bg = NA, pch = par("pch"),cex = 1, col.smooth = 2, span = 2/3, iter = 3)panel.smooth(x, y, col = alpha('orange',0.25), bg = NA, pch = 20,cex = 1, span = 2/3, iter = 3,col.smooth = 'blue'))
```

## Assessment of assumptions

-   We do not observe evidence of heteroscedasticity nor overdispersion
-   The standardised residuals are approximately normal
-   There are no patterns to suggest non-linearity

## Second model

We remove the non-significant variables and fit another model

```{r}
glm_reduced_make <- glm(Price~.-Seller.Type-Engine-Torque-Drivetrain,data=train,family = Gamma(link="log"))
summary(glm_reduced_make)
```

In this model most of the variables are significant

## Comparison

We can compare the two models

```{r}
scores <- data.frame(Model=NA,df=NA,Dev=NA,AIC=NA)
scores[1,] <- list(Model="Full",
                   df = summary(glm_full)$df[1],
                   Dev = summary(glm_full)$deviance,
                   AIC = summary(glm_full)$aic)
scores[2,] <- list(Model="Reduced",
                   df=summary(glm_reduced_make)$df[1],
                   Dev=summary(glm_reduced_make)$deviance,
                   AIC=summary(glm_reduced_make)$aic)
scores
```

Since deviances are very similar and AIC is slightly lower for the reduced model, we shall choose the latter. Moreover, a LRT shows that there is no evidence toward rejecting the simpler model

```{r}
st <- anova(glm_reduced_make,glm_full,"Chisq")
print(paste("p-value =",round(pchisq(2*st$Deviance[2],st$Df[2],lower.tail = F),2)))
```

## Assessment of assumptions

We inspect the residuals from the reduce model to check that our assumptions are met

```{r}
library(ggplot2)
par(mfrow=c(2,2))
plot(glm_reduced_make,pch=20,col=alpha('orange',0.25),panel=function(x, y, col = par("col"), bg = NA, pch = par("pch"),cex = 1, col.smooth = 2, span = 2/3, iter = 3)panel.smooth(x, y, col = alpha('orange',0.25), bg = NA, pch = 20,cex = 1, span = 2/3, iter = 3,col.smooth = 'blue'))
```

```{r,eval=F}
library(ggplot2)
ggplot(train,aes(x=predict(glm_reduced_make),y=glm_reduced_make$residuals))+geom_point(alpha=.25,color='orange')+geom_smooth(color='blue')+geom_hline(aes(yintercept=0),alpha=0.4)
```

## Assessment of assumptions

-   We do not observe evidence of heteroscedasticity nor overdispersion
-   The standardised residuals are approximately normal
-   There are no patterns to suggest non-linearity

## To make or not to make

The variable Make has many levels and thus introduces many degrees of freedom in the model. We might ask if introducing that many degrees of freedom is beneficial for the model.

```{r}
glm_no_make <- glm(Price~.-Make-Seller.Type-Engine-Torque-Drivetrain,data=train,family = Gamma(link="log"))
summary(glm_no_make)
```

## To make or not to make

```{r}
scores[3,] <- list(Model="No make",
                   df=summary(glm_no_make)$df[1],
                   Dev=summary(glm_no_make)$deviance,
                   AIC=summary(glm_no_make)$aic)
scores
```

From a comparison with the first two models we see that both the deviance and AIC increase significantly if we remove Make, thus we are justified in keeping it. Further evidence comes from a LRT comparing the reduced and the no make model.

```{r}
st <- anova(glm_no_make,glm_reduced_make,"Chisq")
print(paste("p-value =",round(pchisq(2*st$Deviance[2],st$Df[2],lower.tail = F),2)))
```

# Comparison

## Similarities

The LM and the GLM perform very similarly:

-   The residuals do not show clear non-linearities nor eteroscedasticity
-   The explained null deviance is $\approx 95\%$
-   The mean absolute percentage error in prediction on new data is 17%
-   The coefficients are mostly the same within one standard error

## Interpretation of parameters

For both models the interpretation is the same: upon increasing the value for a variable by one unit and leaving the others fixed, the price increases or decreases by a factor $exp(\beta)$, according to the sign of the coefficient.

For instance, an increase by 10'000km amounts to a decrease in prie of about 1%.

## Interpretation of parameters

For categorical variables, for each level the coefficient is such that the price increases or decreases by a factor $exp(\beta)$ with respect to the price for the base level.

For instance, Unregistered cars cost 13% more than cars with one previous owner.

## Interpretation of parameters

```{r}
gz <- as.numeric(coef(glm_reduced_make)[2:33]>0)
mks<- sort(unique(train$Make))[-1][coef(glm_reduced_make)[2:33]>0]

price <- character(length=nrow(train))
price[train$Make %in% mks] <- "Higher"
price[!(train$Make %in% mks)] <- "Lower"
price[train$Make == "Audi"] <- "Baseline"

med <- log(median(train[train$Make=="Audi",]$Price))
ggplot(train,aes(x=Make,y=log(Price),col=price,fill=price))+geom_boxplot(alpha=0.4)+geom_hline(aes(yintercept=med))+theme(axis.text.x=element_text(angle = -90, hjust = 0))+scale_fill_manual(values = c("grey","#389129","#ad1d13"))+scale_color_manual(values = c("grey","#389129","#ad1d13"))
```

# Bonus

## Random Forest

-   Performs better in prediction, with an error of $\approx 9\%$
-   Worse interpretability
-   Hard to understand the real model
