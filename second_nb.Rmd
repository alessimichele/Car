---
title: "Second: data v4"
output: html_notebook
---


```{r}
library(ggplot2)
library(GGally)
library(stringr)
library(plotly)
library(MASS)
library(glmnet)
```


```{r}
cor_mat <- round(cor(car[,unlist(lapply(car, is.numeric), use.names = FALSE)]),2)
data<-expand.grid(X=unlist(dimnames(cor_mat)[2]),Y=unlist(dimnames(cor_mat)[2]))
data$Z<- c(cor_mat)

ggplot(data,aes(X,Y,fill=Z))+geom_tile()+theme(axis.text.x=element_text(angle = -90, hjust = 0))+coord_equal()+scale_y_discrete(limits=rev)+geom_text(aes(label=prettyNum(Z,digits=2,format="f")), color="white", size=4)
```



## Importing the data
We have a dataset of 1993 observations, obtained from the original data by removing observations with too many missing values, imputing some other missing values and removing outliers. We transform categorical variables into factors. We then split the dataset in train and test sets.
```{r}
car<-read.csv("car.csv")

# Transform categorical variables into factors
car$Make <- as.factor(car$Make)
car$Fuel.Type <- as.factor(car$Fuel.Type)
car$Transmission <- as.factor(car$Transmission)
car$Owner <- as.factor(car$Owner)
car$Seller.Type<- as.factor(car$Seller.Type)
car$Drivetrain <- as.factor(car$Drivetrain)
car$SeatingCapacity <- as.factor(car$SeatingCapacity)

car <- car[car$Kilometer != 2000,]

set.seed(1)
index <- sample(nrow(car),nrow(car)*0.8,replace = F)
train <- car[index,]
test <- car[-index,]
```







```{r}
## Price distribution

par(mfrow=c(2,2))
price_outliers <- which(car$price> quantile(car$Price,0.99))
boxplot(car$rice[-price_outliers])
hist.scott(car$price[-price_outliers],prob = F)
boxplot(log(car$price[-price_outliers]))
hist.scott(log(car$price[-price_outliers]),prob = F)
```

```{r}
temp <- ndf[price_outliers,]
```

```{r}
## Kilometers distribution

par(mfrow=c(2,2))
km_outliers <- which(car$km> quantile(car$km,0.9999))
boxplot(car$km[-km_outliers])
hist.scott(car$km[-km_outliers],prob = F)
boxplot(sqrt(car$km[-km_outliers]))
hist.scott(sqrt(car$km[-c(km_outliers,1767)]),prob = F)
```



```{r}
scores <- data.frame(model=NA,aic=NA,null.dev=NA,res.dev=NA)
```




# The (mis)fits




We fit a glm with Gamma(log). The choice for a Gamma is justified by the fact that prices are always positive and from the inspection of price vs years it appears that higher prices have larger variance. 
```{r}
glm_full <- glm(Price~.,data=train,family=Gamma(link="log"))
summary(glm_full)
scores <- cbind("full",summary(glm_full)$aic,summary(glm_full)$null.deviance,summary(glm_full)$deviance)

qn <- qqnorm(residuals(glm_full,"pearson")/sqrt(0.04303429),col=2) ## qqplot with dispersion param normalisation
plot(glm_full,which=2) ## qqplot
points(qn$x,qn$y,col=alpha(2,0.4),pch=20)

plot(predict(glm_full,newdata = test,type="link"),log(test$Price))
abline(a=0,b=1,col=2)
```



```{r}
glm_reduced_make <- glm(Price~.-Seller.Type-Engine-Torque-Drivetrain,data=train,family = Gamma(link="log"))
summary(glm_reduced_make)
scores <- rbind(scores,cbind("reduced",summary(glm_reduced_make)$aic,summary(glm_reduced_make)$null.deviance,summary(glm_reduced_make)$deviance))

qn_r <- qqnorm(residuals(glm_reduced_make,"pearson")/sqrt(0.04303429),col=2) ## qqplot with dispersion param normalisation
plot(glm_reduced_make) ## qqplot
points(qn_r$x,qn_r$y,col=alpha(2,0.4),pch=20)

plot(predict(glm_reduced_make,newdata = test,type="link"),log(test$Price))
abline(a=0,b=1,col=2)
```


```{r}
library(car)
vif(glm_reduced_make)

```


```{r}
#par(mfrow=c(4,4))
for(col in colnames(train)){
  plot(train[,col],residuals(glm_reduced_make,"pearson"),main=paste(col))
}

large.res <- which(abs(residuals(glm_reduced_make,"pearson"))>0.5)
poorly.pred <- train[large.res,]
```

```{r}
plot(residuals(glm_reduced_make,"pearson"))
```


```{r}
glm_gauss_reduced_make <- glm(Price~.-Seller.Type-Engine-Torque-Drivetrain,data=train,family = gaussian(link="log"))
summary(glm_gauss_reduced_make)
scores <- rbind(scores,cbind("gauss",summary(glm_gauss_reduced_make)$aic,summary(glm_gauss_reduced_make)$null.deviance,summary(glm_gauss_reduced_make)$deviance))
plot(glm_gauss_reduced_make)
```





### histogram of residuals
```{r}
myhist<-hist.scott(residuals(glm_full,"pearson")/sqrt(0.04303429),prob=F,col=alpha(7,0.4))
n <- length(residuals(glm_full))
p<-dnorm(myhist$mids,0,1)*0.5
sd<-sqrt(n*p*(1-p))
points(myhist$mids,n*p,pch=20,col=4)
segments(myhist$mids,n*p-sd,myhist$mids,n*p+sd,col=4)

dici <- which(myhist$counts >= 30)
chi2 <- sum((myhist$counts[dici]-n*p[dici])^2/sd[dici]^2)
pchisq(chi2,length(dici),lower.tail = F)
```


```{r}
glm_no_make <- glm(Price~.-Make,data=train,family=Gamma(link="log"))
summary(glm_no_make)
scores <- rbind(scores,cbind("no make",summary(glm_no_make)$aic,summary(glm_no_make)$null.deviance,summary(glm_no_make)$deviance))
plot(glm_no_make)
```

## Lasso
```{r}
## build a model matrix
xx <- model.matrix(Price~.-Price,data=train)
```

```{r}
set.seed(1)


glm_lasso_cv <- cv.glmnet(xx,train$Price,alpha=1,family=Gamma(link="log"))
plot(glm_lasso_cv)
glm_lasso <- glmnet(xx,train$Price,alpha=1,family=Gamma(link="log"),lambda = glm_lasso_cv$lambda.1se)
#l0 <- logLik(glm(Price~1,data=train,family=Gamma("log")))
#2*(glmnet:::deviance.glmnet(glm_lasso)-glm_lasso$nulldev) + 2*(glm_lasso$df + 1)-24512*2
coef.glmnet(glm_lasso)
scores <- rbind(scores,cbind("lasso",NA,glm_lasso$nulldev,(1-glm_lasso$dev.ratio)*glm_lasso$nulldev ))
glm_lasso
```

```{r}
glm_step <- step(glm_full)
summary(glm_step)
newrow <- c(model="step", aic=summary(glm_step)$aic,null.dev=summary(glm_step)$null.deviance,deviance=summary(glm_step)$deviance,mare=NA)
scores <- rbind(scores,newrow)
```




### compute mare scores
```{r}
scores <- data.frame(scores)
scores$mare[1] <- sum(abs(predict(glm_full,newdata =  test,type="response")-test$Price)/test$Price)/nrow(test)
scores$mare[2] <- sum(abs(predict(glm_reduced_make,newdata =  test,type="response")-test$Price)/test$Price)/nrow(test)
scores$mare[3] <-sum(abs(predict(glm_gauss_reduced_make,newdata =  test,type="response")-test$Price)/test$Price)/nrow(test)
scores$mare[4] <-sum(abs(predict(glm_no_make,newdata =  test,type="response")-test$Price)/test$Price)/nrow(test)
scores$mare[5] <-sum(abs(predict(glm_lasso,newx = xx_test,type="response")-test$Price)/test$Price)/nrow(test)
scores$mare[6] <-sum(abs(predict(glm_step,newdata = test,type="response")-test$Price)/test$Price)/nrow(test)

colnames(scores) <- c("model","aic","null.dev","dev","mare")
scores$aic <- as.numeric(scores$aic,2)
scores$null.dev <- as.numeric(scores$null.dev,2)
scores$dev <- as.numeric(scores$dev,2)
scores$mare <- as.numeric(scores$mare,2)
```

```{r}

xx_test <- model.matrix(Price~.,data=test)
plot(predict(glm_lasso,newx = xx_test,type="response"),test$Price)
abline(a=0,b=1,col=2)
```





```{r}
xx_nm <- model.matrix(price~.-price-make,data=car)

glm_lasso_cv_nm <- cv.glmnet(xx_nm,car$price,alpha=1,family=Gamma(link="log"))
plot(glm_lasso_cv_nm)
glm_lasso_nm <- glmnet(xx_nm,car$price,alpha=1,family=Gamma(link="log"),lambda = glm_lasso_cv_nm$lambda.1se)
glmnet:::deviance.glmnet(glm_lasso_nm) + 2*(glm_lasso_nm$df + 1)
coef.glmnet(glm_lasso_nm)
```


## Ridge ?
```{r}
set.seed(1)
xx <- model.matrix(Price~.-Price,data=car)

glm_ridge_cv <- cv.glmnet(xx,car$Price,alpha=0,family=Gamma(link="log"))
plot(glm_ridge_cv)
glm_ridge <- glmnet(xx,car$Price,alpha=0,family=Gamma(link="log"),lambda = glm_ridge_cv$lambda.1se)
glmnet:::deviance.glmnet(glm_ridge) + 2*(glm_ridge$df + 1)
glm_ridge
```








## Interaction
```{r}
glm_inter <- glm(Price~Make+Kilometer+Year+Owner+Power, data= train,family=Gamma("log"))
summary(glm_inter)
```







## Plot price vs everything
```{r}
y_pred <- predict(glm_reduced_make,newdata=test,type="response")
for(col in colnames(test)){
  plot(test[,col],test$Price,main=paste(col))
  points(test[,col],y_pred,col=2)
}
```


```{r}
with(car, plot(Price~log(Kilometer),xlim=c(6,12)))
points(y=predict(glm_lasso,newx=xx,type="response"),x=log(car$Kilometer),col=2)
```

